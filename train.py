import os
import time
import math
import numpy as np
from typing import List, Tuple

import torch
import torch.nn as nn
import torch.optim as optim

from gym_env import PointsEnv
from network import AttentionNet

from valid import build_fixed_eval_envs

import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime

import glob


# ========== å¯è°ƒè¶…å‚ ==========
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

GRAPH_TASKS = 100         # æ¯ä¸ªå›¾çš„ä»»åŠ¡ç‚¹æ•°é‡ï¼ˆä½ çš„ env å‚æ•°ï¼‰
MAX_DIST = 3.0             # æ¯ä¸ªå›¾çš„æœ€å¤§é‡Œç¨‹
BATCH_SIZE = 48            # æ¯æ¬¡ update ç”¨ 64 ä¸ªç‹¬ç«‹å›¾/episode
N_EPOCHS = 300             # ä¸€å…±è®­ç»ƒ 100 ä¸ª epoch
UPDATES_PER_EPOCH = 10   # æ¯ä¸ª epoch è¿›è¡Œ 1000 æ¬¡åå‘ä¼ æ’­ï¼ˆæ¯æ¬¡ 64 å›¾ï¼‰

LR = 1e-4
GAMMA = 1
ENTROPY_COEF = 0.0         # ä¸ Kool ä¸»å¹²ä¸€è‡´ï¼Œé€šå¸¸è®¾ 0ï¼›éœ€è¦æ¢ç´¢å¯>0
MAX_GRAD_NORM = 1
BETA = 0.8                 # EMA baseline å¹³æ»‘
VAL_EPISODES = 100         # éªŒè¯æ—¶çš„æ ·æœ¬æ•°é‡ï¼ˆå¯è°ƒï¼‰
SAVE_DIR = "checkpoints_kool_style"
os.makedirs(SAVE_DIR, exist_ok=True)

# ========== å·¥å…·ï¼šå¯¼å…¥æ¨¡å‹ ==========
def find_latest_ckpt(model_dir: str) -> str:
    """åœ¨æŒ‡å®šæ–‡ä»¶å¤¹é‡ŒæŒ‰ä¿®æ”¹æ—¶é—´æ‰¾æœ€æ–°çš„ .pt / .pth"""
    cand = glob.glob(os.path.join(model_dir, "*.pt")) + glob.glob(os.path.join(model_dir, "*.pth"))
    if not cand:
        return None
    cand.sort(key=lambda p: os.path.getmtime(p), reverse=True)
    return cand[0]

def try_resume_from_ckpt(policy: nn.Module,
                         optimizer: optim.Optimizer,
                         baseline: 'ExponentialBaseline',
                         path: str,
                         device: str = "cpu"):   # â† é»˜è®¤ç”¨cpu
    """
    ä» checkpoint æ¢å¤ï¼Œè¿”å› (start_epoch, best_val)ã€‚
    å…ˆå¼ºåˆ¶CPUåŠ è½½ï¼Œé¿å… ROCm ä¸‹ amdsmi åˆå§‹åŒ–é”™è¯¯ï¼›éšåæŠŠæƒé‡å’Œä¼˜åŒ–å™¨çŠ¶æ€è¿å› deviceã€‚
    """
    print(f"ğŸ” Loading checkpoint (CPU-safe): {path}")
    # æ³¨æ„ï¼šä¸ºäº†èƒ½æ¢å¤ optimizerï¼Œæˆ‘ä»¬è¿™é‡Œä»éœ€è¦ weights_only=False
    ckpt = torch.load(path, map_location="cpu")  # â˜… å…³é”®ï¼šå¼ºåˆ¶åˆ°CPU

    # çº¯ state_dict å…¼å®¹
    if isinstance(ckpt, dict) and ("model" not in ckpt) and \
       ("state_dict" in ckpt or any(isinstance(v, torch.Tensor) for v in ckpt.values())):
        state = ckpt.get("state_dict", ckpt)
        policy.load_state_dict(state)
        policy.to(device)
        print("  âœ… Loaded model weights (state_dict only).")
        return 0, -1e9

    # æ ‡å‡†ä¿å­˜æ ¼å¼
    if "model" in ckpt:
        policy.load_state_dict(ckpt["model"])
        policy.to(device)
        print("  âœ… Loaded model weights.")

    # æ¢å¤ä¼˜åŒ–å™¨ï¼ˆå…ˆåœ¨CPUä¸Šloadï¼Œå†æŠŠstateé‡Œçš„å¼ é‡è¿åˆ°deviceï¼‰
    if "opt" in ckpt:
        try:
            optimizer.load_state_dict(ckpt["opt"])
            # æŠŠä¼˜åŒ–å™¨stateé‡Œçš„å¼ é‡æ¬åˆ°ç›®æ ‡device
            for state in optimizer.state.values():
                for k, v in state.items():
                    if isinstance(v, torch.Tensor):
                        state[k] = v.to(device)
            # è¦†ç›–å­¦ä¹ ç‡ä¸ºå½“å‰è„šæœ¬è®¾å®š
            for pg in optimizer.param_groups:
                pg["lr"] = LR
            print(f"  âœ… Restored optimizer (moved to {device}, lr={LR}).")
        except Exception as e:
            print(f"  âš ï¸ Optimizer state not loaded: {e}")

    # æ¢å¤ EMA baselineï¼ˆæ ‡é‡ï¼‰
    if "baseline" in ckpt:
        try:
            baseline._b = torch.tensor(float(ckpt["baseline"]), dtype=torch.float32, device=device)
            print(f"  âœ… Restored baseline EMA: {baseline.value():.4f}")
        except Exception as e:
            print(f"  âš ï¸ Baseline not restored: {e}")

    start_epoch = int(ckpt.get("epoch", 0))
    best_val = float(ckpt.get("best_val", -1e9))
    print(f"  â–¶ï¸ Resume from epoch={start_epoch}, best_val={best_val:.4f}")
    return start_epoch, best_val


# ===== Resume / Load =====
MODEL_DIR = "model"          # ä½ å­˜æ”¾å·²è®­ç»ƒæ¨¡å‹çš„æ–‡ä»¶å¤¹
RESUME = True                # True=å°è¯•ç»§ç»­è®­ç»ƒï¼›False=ä»å¤´å¼€å§‹
# RESUME_PATH = None           # æ‰‹åŠ¨æŒ‡å®š ckpt è·¯å¾„ï¼ˆä¾‹å¦‚ "model/10-17-best_model_384_5000.pt"ï¼‰
#                              # è‹¥ä¸º None ä¸” RESUME=Trueï¼Œåˆ™è‡ªåŠ¨åœ¨ MODEL_DIR æŒ‰ä¿®æ”¹æ—¶é—´æ‰¾æœ€æ–° *.pt/*.pth

RESUME_PATH = "model/10-21/10-21-last_model_900_560.pt"


# ========== å·¥å…·ï¼šç»˜å›¾ ==========
def ensure_dir(d):
    os.makedirs(d, exist_ok=True)

def now_tag():
    # å½¢å¦‚ 10_19_1935ï¼ˆMM_DD_HHMMï¼‰
    return datetime.now().strftime("%m_%d_%H%M")

def init_result_dir():
    base_dir = os.path.join("result_cur", f"train_{now_tag()}")
    ensure_dir(base_dir)
    print(f"ğŸ“ results will be saved under: {base_dir}")
    return base_dir

def save_excels(base_dir, train_rows, eval_rows):
    """æŒç»­è¦†ç›–å¼ä¿å­˜åˆ°ä¸¤ä»½ Excelã€‚"""
    df_train = pd.DataFrame(train_rows)  # åˆ—: epoch, update, reward, loss
    df_eval  = pd.DataFrame(eval_rows)   # åˆ—: epoch, val_return, eval_time, train_time
    # ä¸¤ä¸ª Excelï¼šä¸€ä¸ªè¯„æµ‹ç»“æœ&è€—æ—¶ï¼Œä¸€ä¸ªè®­ç»ƒ reward/loss
    eval_xlsx  = os.path.join(base_dir, "eval.xlsx")
    train_xlsx = os.path.join(base_dir, "train.xlsx")
    # ä½¿ç”¨ openpyxl å†™ xlsxï¼ˆpandas é»˜è®¤å³å¯ï¼‰
    df_eval.to_excel(eval_xlsx, index=False)
    df_train.to_excel(train_xlsx, index=False)
    return eval_xlsx, train_xlsx

def plot_curves(base_dir, train_rows, eval_rows):
    """è®­ç»ƒç»“æŸï¼ˆæˆ–ä¸­æ–­ï¼‰åç”» 4 å¼ å›¾ï¼šval_returnã€eval_timeã€epochå‡å€¼rewardã€epochå‡å€¼lossã€‚"""
    if len(eval_rows) > 0:
        df_eval = pd.DataFrame(eval_rows)
        # 1) val_return
        plt.figure()
        plt.plot(df_eval["epoch"], df_eval["val_return"])
        plt.xlabel("epoch"); plt.ylabel("val_return (greedy on fixed-100)")
        plt.title("Validation Return over Epochs")
        plt.grid(True, alpha=0.3)
        plt.savefig(os.path.join(base_dir, "val_return_curve.png"), dpi=150)
        plt.close()

        # 2) eval_time
        plt.figure()
        plt.plot(df_eval["epoch"], df_eval["eval_time"])
        plt.xlabel("epoch"); plt.ylabel("eval_time (s)")
        plt.title("Evaluation Time over Epochs")
        plt.grid(True, alpha=0.3)
        plt.savefig(os.path.join(base_dir, "eval_time_curve.png"), dpi=150)
        plt.close()

    if len(train_rows) > 0:
        df_train = pd.DataFrame(train_rows)  # æ¯ä¸ª update ä¸€è¡Œ
        # èšåˆåˆ° epoch å‡å€¼ï¼Œé¿å…æ›²çº¿å¤ªæŠ–
        df_ep = df_train.groupby("epoch", as_index=False).agg(
            mean_reward=("reward", "mean"),
            mean_loss=("loss", "mean")
        )

        # 3) mean_reward per epoch
        plt.figure()
        plt.plot(df_ep["epoch"], df_ep["mean_reward"])
        plt.xlabel("epoch"); plt.ylabel("mean_reward (per-epoch avg)")
        plt.title("Training Mean Reward per Epoch")
        plt.grid(True, alpha=0.3)
        plt.savefig(os.path.join(base_dir, "train_mean_reward_curve.png"), dpi=150)
        plt.close()

        # 4) mean_loss per epoch
        plt.figure()
        plt.plot(df_ep["epoch"], df_ep["mean_loss"])
        plt.xlabel("epoch"); plt.ylabel("mean_loss (per-epoch avg)")
        plt.title("Training Mean Loss per Epoch")
        plt.grid(True, alpha=0.3)
        plt.savefig(os.path.join(base_dir, "train_mean_loss_curve.png"), dpi=150)
        plt.close()

# ========== å·¥å…·ï¼šæŒ‡æ•°æ»‘åŠ¨åŸºçº¿ ==========
class ExponentialBaseline:
    def __init__(self, beta: float = 0.8):
        self.beta = beta
        self._b = None

    @torch.no_grad()
    def eval_and_update(self, returns: torch.Tensor) -> torch.Tensor:
        """returns: [B]ï¼Œè¿”å›æ ‡é‡ baselineï¼ˆå¸¸æ•°åŸºçº¿ï¼Œä¸é€æ ·æœ¬ï¼‰"""
        val = returns.mean()  # æ‰¹å‡å€¼
        if self._b is None:
            self._b = val
        else:
            self._b = self.beta * self._b + (1 - self.beta) * val
        return self._b

    def value(self) -> float:
        return float(self._b) if self._b is not None else 0.0

# ========== å•ä¸ª env ä¸Šæ»šåŠ¨ä¸€ä¸ª episodeï¼Œè¿”å›â€œåºåˆ— logp ä¹‹å’Œâ€å’Œâ€œæŠ˜æ‰£å›æŠ¥â€ ==========
def run_one_episode_collect_seq(policy: AttentionNet, env: PointsEnv) -> Tuple[torch.Tensor, torch.Tensor, float]:
    """
    è¿”å›:
      seq_logp_sum: shape [], å½“å‰ç­–ç•¥åœ¨æ•´æ¡è½¨è¿¹ä¸Šçš„ logÏ€ ä¹‹å’Œï¼ˆæ ‡é‡å¼ é‡ï¼‰
      Rt:           shape [], è¯¥æ¡è½¨è¿¹çš„æŠ˜æ‰£å›æŠ¥ï¼ˆæ ‡é‡å¼ é‡ï¼‰
      ep_return:    float, æœªæŠ˜æ‰£æ€»å›æŠ¥ï¼ˆç”¨äºæ‰“å°ç»Ÿè®¡ï¼‰
    è¯´æ˜ï¼š
      - æ•´æ¡è½¨è¿¹æŒ‰â€œé‡‡æ ·â€é€‰åŠ¨ä½œ
      - æ¯ä¸€æ­¥å°† log_prob(action) ç´¯åŠ 
      - æŠ˜æ‰£å›æŠ¥ Rt ç”¨ GAMMA è®¡ç®—
    """
    env.reset(env.world.rewards, env.world.start_depot_index)
    done = False

    step_logps: List[torch.Tensor] = []
    step_rewards: List[float] = []
    ep_return = 0.0

    while not done:
        points, agent_idx, remaining_distance, valid_mask = env.observe()
        # ç»„è£…å¼ é‡ï¼ˆå•æ¡è½¨è¿¹ï¼Œç”¨ batch ç»´åº¦=1ï¼‰
        points_t = torch.tensor(points, dtype=torch.float32, device=DEVICE).unsqueeze(0)            # [1, N, 3]
        agent_t  = torch.tensor([agent_idx], dtype=torch.long, device=DEVICE)                       # [1]
        rem_t    = torch.tensor([[remaining_distance]], dtype=torch.float32, device=DEVICE)         # [1, 1]
        mask_t   = torch.tensor(valid_mask, dtype=torch.float32, device=DEVICE).unsqueeze(0)        # [1, N]

        # å‰å‘å¾—åˆ°åˆ†å¸ƒï¼ˆè®­ç»ƒ=é‡‡æ ·ï¼‰
        dist = policy(points_t, agent_t, rem_t, mask_t.bool())
        action = dist.sample()                                 # [1]
        logp   = dist.log_prob(action).squeeze(0)              # []
        step_logps.append(logp)

        action_int = int(action.item())
        _, reward, done = env.step(action_int)

        step_rewards.append(float(reward))
        ep_return += float(reward)

    # æŠ˜æ‰£å›æŠ¥ï¼ˆreward-to-go çš„é¦–é¡¹ï¼Œç­‰ä»·æ•´æ¡æŠ˜æ‰£å’Œï¼‰
    Rt = 0.0
    for r in reversed(step_rewards):
        Rt = r + GAMMA * Rt
    Rt = torch.tensor(Rt, dtype=torch.float32, device=DEVICE)  # []

    # åºåˆ— log æ¦‚ç‡ä¹‹å’Œ
    seq_logp_sum = torch.stack(step_logps).sum()               # []

    return seq_logp_sum, Rt, ep_return

# ========== é‡‡æ ·ä¸€ä¸ª batchï¼ˆ64 æ¡ç‹¬ç«‹å›¾ï¼‰å¹¶è®¡ç®—ä¸€æ¬¡ REINFORCE æŸå¤± ==========
def reinforce_update(policy: AttentionNet, optimizer, baseline: ExponentialBaseline) -> Tuple[float, float]:
    """
    è¿›è¡Œä¸€æ¬¡å‚æ•°æ›´æ–°ï¼ˆä¸€æ¬¡åå‘ä¼ æ’­ï¼‰ï¼ŒåŸºäº BATCH_SIZE=64 æ¡ç‹¬ç«‹å›¾/episodeã€‚
    è¿”å›:
      mean_cost:  æœ¬æ¬¡ batch æœªæŠ˜æ‰£æ€»å›æŠ¥çš„è´Ÿå€¼å‡å€¼ï¼ˆå½“ä½œâ€œä»£ä»·â€ï¼Œä»…ç»Ÿè®¡ï¼‰
      loss_val:   æœ¬æ¬¡ç”¨äºåå‘ä¼ æ’­çš„ loss æ ‡é‡ï¼ˆfloatï¼‰
    """
    policy.train()
    seq_logps = []
    returns = []
    ep_returns = []

    # ä¸ºåŠ é€Ÿï¼Œè¿™é‡Œé‡å¤ä½¿ç”¨ 64 ä¸ªç‹¬ç«‹ envï¼Œæ¯ä¸ª env åªè·‘ä¸€æ¡è½¨è¿¹
    envs = [PointsEnv(tasks_number=GRAPH_TASKS, max_distance=MAX_DIST) for _ in range(BATCH_SIZE)]

    for env in envs:
        seq_logp_sum, Rt, ep_ret = run_one_episode_collect_seq(policy, env)
        seq_logps.append(seq_logp_sum)  # []
        returns.append(Rt)              # []
        ep_returns.append(ep_ret)

    seq_logps_t = torch.stack(seq_logps)    # [B]
    returns_t   = torch.stack(returns)      # [B]

    # åŸºçº¿ï¼ˆå¸¸æ•°åŸºçº¿ï¼‰ï¼šEMA(returns.mean)
    b = baseline.eval_and_update(returns_t)  # æ ‡é‡
    adv = returns_t - b                      # [B]

    # å¯é€‰ï¼šæ ‡å‡†åŒ– advantageï¼ˆé€šå¸¸å¯¹ç¨³å®šè®­ç»ƒæœ‰ç›Šï¼‰
    adv = (adv - adv.mean()) / (adv.std() + 1e-8)

    # REINFORCEï¼š ((R - b) * logÏ€).mean()ï¼Œæˆ‘ä»¬æœ€å°åŒ– -è¯¥æœŸæœ›
    reinforce_loss = -(adv * seq_logps_t).mean()

    # å¯é€‰ï¼šåŠ ç†µæ­£åˆ™ï¼ˆè®ºæ–‡ä¸»å¹²é€šå¸¸ä¸ç”¨ï¼‰
    # è¿™é‡Œæ²¡æœ‰é€æ­¥ entropyï¼Œæ‰€ä»¥å…ˆä¸åŠ ï¼›è‹¥ä½ è¦åŠ ï¼Œåœ¨æ”¶é›†æ—¶æŠŠ step entropy ç´¯åŠ ä¸€èµ·å¹³å‡å³å¯ã€‚
    loss = reinforce_loss

    optimizer.zero_grad()
    loss.backward()
    nn.utils.clip_grad_norm_(policy.parameters(), MAX_GRAD_NORM)
    optimizer.step()

    mean_cost = float(np.mean(ep_returns))  # ä»…ç»Ÿè®¡æ˜¾ç¤ºï¼ˆæŠŠâ€œå›æŠ¥â€å–è´Ÿï¼Œå½“ä½œ costï¼‰
    return mean_cost, float(loss.item())

# ========== è´ªå¿ƒè¯„ä¼° ==========
#@torch.no_grad()
# def evaluate_greedy(policy: AttentionNet, n_eval: int = VAL_EPISODES) -> float:
#     """
#     ç”¨è´ªå¿ƒç­–ç•¥ï¼ˆargmaxï¼‰åœ¨ n_eval ä¸ªç‹¬ç«‹å›¾ä¸Šè¯„ä¼°å¹³å‡â€œæœªæŠ˜æ‰£æ€»å›æŠ¥â€ã€‚
#     """
#     policy.eval()
#     rets = []
#     for _ in range(n_eval):
#         env = PointsEnv(tasks_number=GRAPH_TASKS, max_distance=MAX_DIST)
#         env.reset(env.world.rewards, env.world.start_depot_index)
#         done = False
#         ep_ret = 0.0
#         while not done:
#             points, agent_idx, remaining_distance, valid_mask = env.observe()
#             points_t = torch.tensor(points, dtype=torch.float32, device=DEVICE).unsqueeze(0)
#             agent_t  = torch.tensor([agent_idx], dtype=torch.long, device=DEVICE)
#             rem_t    = torch.tensor([[remaining_distance]], dtype=torch.float32, device=DEVICE)
#             mask_t   = torch.tensor(valid_mask, dtype=torch.float32, device=DEVICE).unsqueeze(0)
#             dist = policy(points_t, agent_t, rem_t, mask_t.bool())
#             action = torch.argmax(dist.probs, dim=-1).item()
#             _, r, done = env.step(int(action))
#             ep_ret += float(r)
#         rets.append(ep_ret)
#     return float(np.mean(rets))

# ===== ç”¨å›ºå®š 100 å¼ å›¾åšè´ªå¿ƒè¯„æµ‹ =====
@torch.no_grad()
def evaluate_greedy_fixed(policy: AttentionNet, eval_envs) -> float:
    """
    åœ¨ä¼ å…¥çš„å›ºå®šè¯„æµ‹ç¯å¢ƒåˆ—è¡¨ eval_envs ä¸Šåšè´ªå¿ƒè¯„æµ‹ã€‚
    æ¯æ¬¡è¯„æµ‹éƒ½ä¼šæŠŠå„è‡ªç¯å¢ƒ reset å›åˆ°åŒä¸€å¼ å›¾ï¼ˆåŒå¥–åŠ±&èµ·ç‚¹ï¼‰ã€‚
    """
    policy.eval()
    rets = []
    for env in eval_envs:
        # ç¡®ä¿å›åˆ°åŒä¸€å¼ å›¾ï¼šä½¿ç”¨è¯¥ env è‡ªå·±çš„ world å‚æ•°é‡ç½®
        env.reset(env.world.rewards, env.world.start_depot_index)

        done = False
        ep_ret = 0.0
        while not done:
            points, agent_idx, remaining_distance, valid_mask = env.observe()

            points_t = torch.tensor(points, dtype=torch.float32, device=DEVICE).unsqueeze(0)
            agent_t  = torch.tensor([agent_idx], dtype=torch.long, device=DEVICE)
            rem_t    = torch.tensor([[remaining_distance]], dtype=torch.float32, device=DEVICE)
            mask_t   = torch.tensor(valid_mask, dtype=torch.float32, device=DEVICE).unsqueeze(0)

            dist = policy(points_t, agent_t, rem_t, mask_t.bool())
            #action = dist.sample()
            action = torch.argmax(dist.probs, dim=-1).item()

            _, r, done = env.step(int(action))
            ep_ret += float(r)

        rets.append(ep_ret)
    return float(np.mean(rets))


# ========== ä¸»è®­ç»ƒ ==========
# def main():
#     torch.backends.cudnn.deterministic = True
#
#     policy = AttentionNet().to(DEVICE)
#     optimizer = optim.Adam(policy.parameters(), lr=LR)
#     baseline = ExponentialBaseline(beta=BETA)
#
#     best_val = -1e9
#     ckpt_best = os.path.join(SAVE_DIR, "best.pt")
#
#     #100eval_envs
#     eval_envs = build_fixed_eval_envs(n_envs=100, tasks=GRAPH_TASKS, max_dist=MAX_DIST, seed=2025)
#
#     val_results = []
#     val_times = []
#
#     for epoch in range(1, N_EPOCHS + 1):
#         t0 = time.time()
#         loss_meter = []
#         cost_meter = []
#
#         # â€”â€” æ¯ä¸ª epoch åš 1000 æ¬¡æ›´æ–°ï¼Œæ¯æ¬¡åŸºäº 64 ä¸ªç‹¬ç«‹å›¾ â€”â€” #
#         for u in range(1, UPDATES_PER_EPOCH + 1):
#             mean_reward, loss_val = reinforce_update(policy, optimizer, baseline)
#             loss_meter.append(loss_val)
#             cost_meter.append(mean_reward)
#
#             # ä¹Ÿå¯ä»¥æ¯éš”ä¸€å®šæ­¥æ•°æ‰“å°ä¸€æ¬¡
#             if u % 1 == 0:
#                 print(f"update{u:4d}/{UPDATES_PER_EPOCH}, reward={mean_reward}  loss={loss_val:.4f}")
#
#         # éªŒè¯ï¼ˆè´ªå¿ƒï¼‰
#         t_dur = time.time() - t0
#
#         t0_v = time.time()
#         #val_ret = evaluate_greedy(policy, n_eval=VAL_EPISODES)
#         val_ret = evaluate_greedy_fixed(policy, eval_envs)
#         v_dur = time.time() - t0_v
#
#         val_results.append(val_ret)
#         val_times.append(v_dur)
#
#
#         print(
#             f"[Epoch {epoch:03d}/{N_EPOCHS}] "
#               f"updates={UPDATES_PER_EPOCH}  "
#               f"train_loss={np.mean(loss_meter):.4f}  "
#               f"train_reward(neg_ret)={np.mean(cost_meter):.2f}  "
#               f"val_return(greedy)={val_ret:.2f}  "
#               f"baseline={baseline.value():.2f}  "
#               )
#
#         print(
#               f"train_one_epoch_time={t_dur:.2f}   "
#               f"validate_one_epoch_time={v_dur:.2f}"
#         )
#
#
#         # ä¿å­˜æœ€å¥½
#         if val_ret > best_val:
#             best_val = val_ret
#             torch.save({"model": policy.state_dict(),
#                         "opt": optimizer.state_dict(),
#                         "baseline": baseline.value(),
#                         "epoch": epoch}, ckpt_best)
#             print(f"  âœ… Saved best to {ckpt_best} (val_return={best_val:.2f})")
#
#         print(f"best_val={best_val:.2f}")
#
#     # æœŸæœ«å†å­˜ä¸€ä»½
#     ckpt_last = os.path.join(SAVE_DIR, "last.pt")
#     torch.save({"model": policy.state_dict(),
#                 "opt": optimizer.state_dict(),
#                 "baseline": baseline.value(),
#                 "epoch": N_EPOCHS}, ckpt_last)
#     print(f"Training done. Last saved to {ckpt_last}")
#     print(f"best_val={best_val:.2f}")

def main():
    torch.backends.cudnn.deterministic = True

    # ============ ç»“æœç›®å½• ============
    BASE_DIR = init_result_dir()
    ckpt_best = os.path.join(SAVE_DIR, "best.pt")  # ä½ åŸæœ‰ç›®å½•ä¹Ÿä¿ç•™
    ckpt_last = os.path.join(SAVE_DIR, "last.pt")

    # ============ åˆå§‹åŒ–ç»„ä»¶ ============
    policy = AttentionNet().to(DEVICE)
    optimizer = optim.Adam(policy.parameters(), lr=LR)
    baseline = ExponentialBaseline(beta=BETA)

    # å›ºå®š 100 å¼ å›¾ï¼ˆä½ å·²æœ‰ï¼‰
    eval_envs = build_fixed_eval_envs(n_envs=200, tasks=GRAPH_TASKS, max_dist=MAX_DIST, seed=2025)

    # â€”â€” Resume / ç»§ç»­è®­ç»ƒ â€”â€” #
    start_epoch = 0
    best_val = -1e9
    if RESUME:
        load_path = RESUME_PATH if RESUME_PATH is not None else find_latest_ckpt(MODEL_DIR)
        if load_path is not None and os.path.exists(load_path):
            start_epoch, best_val = try_resume_from_ckpt(policy, optimizer, baseline, load_path, device=DEVICE)
        else:
            print(f"â„¹ï¸ No checkpoint found to resume (looked at {RESUME_PATH or MODEL_DIR}). Start fresh.")

    # ============ æ—¥å¿—ç¼“å­˜ï¼ˆå†…å­˜é‡Œï¼‰ ============
    # è®­ç»ƒåˆ†å¸ƒï¼šæ¯ä¸ª update è®°ä¸€è¡Œ
    train_rows = []  # {epoch, update, reward, loss}
    # è¯„æµ‹ï¼šæ¯ä¸ª epoch è®°ä¸€è¡Œ
    eval_rows  = []  # {epoch, val_return, eval_time, train_time}

    try:
        for epoch in range(1, N_EPOCHS + 1):
            t0_train = time.time()
            loss_meter = []
            reward_meter = []

            # â€”â€” æ¯ä¸ª epoch åš 1000 æ¬¡æ›´æ–°ï¼Œæ¯æ¬¡åŸºäº 64 ä¸ªç‹¬ç«‹å›¾ â€”â€” #
            for u in range(1, UPDATES_PER_EPOCH + 1):
                mean_reward, loss_val = reinforce_update(policy, optimizer, baseline)
                loss_meter.append(loss_val)
                reward_meter.append(mean_reward)

                # è®­ç»ƒåˆ†å¸ƒï¼šé€ update è®°å½•ä¸€è¡Œ
                train_rows.append({
                    "epoch": epoch,
                    "update": u,
                    "reward": float(mean_reward),
                    "loss": float(loss_val),
                })

                # æ§åˆ¶å°æ‰“å°é¢‘ç‡ï¼ˆå¯è°ƒï¼‰
                if u % 1 == 0:
                    print(f"update {u:4d}/{UPDATES_PER_EPOCH}, reward={mean_reward:.4f}  loss={loss_val:.4f}")

            train_dur = time.time() - t0_train

            # â€”â€” è¯„æµ‹ï¼ˆå›ºå®š100å›¾ï¼Œè´ªå¿ƒï¼‰â€”â€”
            t0_eval = time.time()
            val_ret = evaluate_greedy_fixed(policy, eval_envs)
            eval_dur = time.time() - t0_eval

            # è¯„æµ‹æ—¥å¿—ï¼šæ¯ä¸ª epoch ä¸€è¡Œ
            eval_rows.append({
                "epoch": epoch,
                "val_return": float(val_ret),
                "eval_time": float(eval_dur),
                "train_time": float(train_dur),
            })

            # â€”â€” æ§åˆ¶å°ç»Ÿè®¡ â€”â€”
            print(
                f"[Epoch {epoch:03d}/{N_EPOCHS}] "
                f"updates={UPDATES_PER_EPOCH}  "
                f"train_loss(avg)={np.mean(loss_meter):.4f}  "
                f"train_reward(avg)={np.mean(reward_meter):.4f}  "
                f"val_return(greedy)={val_ret:.4f}  "
                f"baseline(ema)={baseline.value():.4f}  "
            )

            print(f"train_time={train_dur:.2f}s  eval_time={eval_dur:.2f}s")

            # â€”â€” ä¿å­˜æœ€å¥½ï¼ˆæŒ‰ val_ret è¶Šå¤§è¶Šå¥½ï¼‰â€”â€”
            if val_ret > best_val:
                best_val = val_ret
                torch.save({
                    "model": policy.state_dict(),
                    "opt": optimizer.state_dict(),
                    "baseline": baseline.value(),
                    "epoch": epoch,
                    "best_val": best_val,
                }, ckpt_best)
                print(f"  âœ… Saved best to {ckpt_best} (val_return={best_val:.4f})")

            print(f"best_val={best_val:.4f}")

            # â€”â€” æ¯ä¸ª epoch ç»“æŸå°±è½ç›˜ä¸€æ¬¡ Excelï¼ŒæŠ—ä¸­æ–­ â€”â€”
            save_excels(BASE_DIR, train_rows, eval_rows)

    except KeyboardInterrupt:
        print("\nâš ï¸ Training interrupted by user (KeyboardInterrupt). Saving partial results...")

    finally:
        # â€”â€” æœŸæœ«ï¼ˆæˆ–ä¸­æ–­ï¼‰ä¿å­˜ last æ¨¡å‹ â€”â€”
        torch.save({
            "model": policy.state_dict(),
            "opt": optimizer.state_dict(),
            "baseline": baseline.value(),
            "epoch": min(len(eval_rows), N_EPOCHS),
            "best_val": best_val,
        }, ckpt_last)
        print(f"ğŸ“¦ Last checkpoint saved to {ckpt_last}")

        # â€”â€” ç¡®ä¿æŠŠ Excel è½ç›˜ â€”â€”
        eval_xlsx, train_xlsx = save_excels(BASE_DIR, train_rows, eval_rows)
        print(f"ğŸ“‘ Excel saved: \n  - {eval_xlsx}\n  - {train_xlsx}")

        # â€”â€” ç”»æ›²çº¿ PNG â€”â€”
        plot_curves(BASE_DIR, train_rows, eval_rows)
        print(f"ğŸ“ˆ Curves saved under: {BASE_DIR}")


if __name__ == "__main__":
    main()
