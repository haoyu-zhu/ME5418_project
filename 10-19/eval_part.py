import torch
import numpy as np
from gym_env import PointsEnv
#from attention_net import AttentionNet
from network import AttentionNet
import time

# ======= å‚æ•° =======
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
#MODEL_PATH = "checkpoints/best_model_27.pt"     # æ¨¡åž‹è·¯å¾„
MODEL_PATH = "checkpoints/10-18/best_model_25.pt"     # æ¨¡åž‹è·¯å¾„
TASK_NUM = 100
MAX_DIST = 3.0
TEST_EPISODES = 100       # æŽ§åˆ¶æµ‹è¯•æ¬¡æ•°ï¼š1=æ¸²æŸ“æ¨¡å¼ï¼Œ>1=å¤šæ¬¡è¯„ä¼°æ¨¡å¼ï¼ˆä¸æ¸²æŸ“ï¼‰
time_start = time.time()
# ======= åŠ è½½ç­–ç•¥ç½‘ç»œ =======
policy = AttentionNet().to(DEVICE)
policy.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
policy.eval()
print(f"âœ… æ¨¡åž‹å·²åŠ è½½: {MODEL_PATH}")

# ======= å®šä¹‰å•æ¬¡æµ‹è¯•å‡½æ•° =======
def run_one_episode(env, render=False):
    env.reset()
    reward_sum = 0.0
    done = False

    if render:
        env.render()

    while not done:
        points, agent_idx, remaining_distance, valid_mask = env.observe()

        points = torch.tensor(points, dtype=torch.float32, device=DEVICE).unsqueeze(0)  # [1, N, 3]
        agent_idx = torch.tensor([agent_idx], dtype=torch.int64, device=DEVICE)  # [1]
        remaining_distance = torch.tensor([[remaining_distance]], dtype=torch.float32, device=DEVICE)  # [1, 1]
        valid_mask = torch.tensor(valid_mask, dtype=torch.bool, device=DEVICE).unsqueeze(0)  # [1, N]

        with torch.no_grad():
            dist = policy(points, agent_idx, remaining_distance, valid_mask)
            action = torch.argmax(dist.probs, dim=-1).item()

        # é˜²æ­¢è¿‡æ—©å›žåˆ°ä»“åº“
        valid_actions = env.list_valid_actions()
        # while len(valid_actions) > 1 and action <= 3:
        #     action = np.random.choice(valid_actions)

        obs, reward, done = env.step(action)
        reward_sum += reward

        if render:
            env.render()

    return reward_sum, env.remaining_distance

# ======= æ‰§è¡Œæµ‹è¯• =======w
env = PointsEnv()
if TEST_EPISODES == 1:
    reward_sum, remaining = run_one_episode(env, render=True)
    print(f"ðŸ å•æ¬¡æµ‹è¯•å®Œæˆ: æ€»å¥–åŠ± = {reward_sum:.2f}, å‰©ä½™è·ç¦» = {remaining:.2f}")

else:
    rewards = []
    for i in range(TEST_EPISODES):
        reward_sum, remaining = run_one_episode(env, render=False)
        #print(reward_sum)
        rewards.append(reward_sum)
        print(f"[{i+1}/{TEST_EPISODES}] Reward = {reward_sum:.2f}, Remaining = {remaining:.2f}")

    avg_reward = np.mean(rewards)
    std_reward = np.std(rewards)
    time_end = time.time()
    print(f"\nâœ… {TEST_EPISODES} æ¬¡æµ‹è¯•å®Œæˆ: å¹³å‡å¥–åŠ± = {avg_reward:.2f} Â± {std_reward:.2f} æ€»è€—æ—¶ = {time_end-time_start:.2f}")